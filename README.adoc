= Climate Informatics Conference Hackathon 2020
:imagesdir: ./media
:icons: font
:my_name: DÃ¡nnell Quesada
:my_email: dannell.quesada@outlook.com
:source-highlighter: pygments
:pygments-linenums-mode: inline
:bl: pass:[ +]
:sectnumlevels: 3
:toc: macro
:toc-title: Contents
:toclevels: 3

This repository contains the solution of the *_Late Birds_* team for the 2020 Climate Informatics Data Challenge, ranked 2^nd^ place. We are mailto:{my_email}[{my_name}] and Aris Marcolongo.

---
toc::[]
---

== About the challenge

The original documentation of the challenge, original Jupyter Notebook and test dataset is https://competitions.codalab.org/my/datasets/download/791b6fe7-de61-45ae-8890-05270324da19[here]. The goal of the Hackathon was to reproduce the visible spectrum (RGB) of satellite images by using the infra-red channels, both taken at the same time, so we can predict visible images (RGB) of Earth at night.

Check the webpage of the https://ci2020.web.ox.ac.uk[Conference] for more details and info.

== About the solution

=== Description

The following is a short description we did about our solution:

[quote]
____
As an image to image model we started from the well-known U-net, adapted it to be compatible with the image dimensions and used 1 - ssim as a custom loss function. The data was preprocessed neglecting images which contained more than 20% of black pixels in the True Color dataset. All data was scaled between 0 and 1 and a sigmoid final activation consistently used. For regularization we used early stopping (90/10 -- train/validation split).The best U-net was selected by trial and error looking at the validation error and the best models had 6 levels of depths, 32 as channel seed and a filter width of 3 pixels. We used Adam as optimizer with a constant learning_rate of 0.001.
____

For more details check the `latebirds.ipynb` notebook, some explanation is added, or you can get in contact with us. Note that the `20%` was changed to `2.5%` in the latest solution, with no major breakthrough, according to the score. But the animations look a bit better.

=== Code

To see the output from our latest solution, with some comments and explanation, open the `latebirds.ipynb` notebook. For the code only, check `latebirds.py`

With the following command, you can reproduce our last result.

[source, python]
----
model, hist, CT, TC = create_and_run_model(epochs=100, batch_size=16, to_remove=0.975, n_layers=6, channel_seed=32, kernel_size=3, validation_split=0.1, run=4)
----

=== Resulting animation

Just as an example, this is an animation of our latest solution.

image::test_pred_naive-0.9386.gif[width=900, height=600]

=== Model visualization

image::model.png[]

== Further details

During the Hackathon, the models were trained in the HPC of TU Dresden. Specifically, on the `machine learning` https://doc.zih.tu-dresden.de/hpc-wiki/bin/view/Compendium/Power9[node]. Further runs were done locally, as in the notebook, to avoid complications getting the animations.
